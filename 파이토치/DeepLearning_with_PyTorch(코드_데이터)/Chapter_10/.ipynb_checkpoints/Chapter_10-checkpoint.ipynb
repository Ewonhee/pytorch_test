{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[코드 10.2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/10]: D_loss: 0.0329, G_loss: 5.5229\n",
      "Epoch [1/10]: D_loss: 0.0193, G_loss: 5.6498\n",
      "Epoch [2/10]: D_loss: 0.5421, G_loss: 4.6169\n",
      "Epoch [3/10]: D_loss: 0.3044, G_loss: 5.0722\n",
      "Epoch [4/10]: D_loss: 0.4379, G_loss: 3.3249\n",
      "Epoch [5/10]: D_loss: 0.0674, G_loss: 4.0474\n",
      "Epoch [6/10]: D_loss: 0.1099, G_loss: 5.5388\n",
      "Epoch [7/10]: D_loss: 0.0583, G_loss: 6.4264\n",
      "Epoch [8/10]: D_loss: 0.3589, G_loss: 4.4488\n",
      "Epoch [9/10]: D_loss: 0.1740, G_loss: 5.5944\n"
     ]
    }
   ],
   "source": [
    "# prerequisites\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 100\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 10\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "\n",
    "# Data Loader \n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())\n",
    "\n",
    "D.to(device)\n",
    "G.to(device)\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0002 \n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n",
    "\n",
    "D_loss_epochs, G_loss_epochs = [], []\n",
    "for epoch in range(num_epochs):    \n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(device)\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "        # ======================================================== #\n",
    "        #                      Train the discriminator             #\n",
    "        # ======================================================== #\n",
    "\n",
    "        outputs = D(images)\n",
    "        D_loss_real = loss_function(outputs, real_labels)\n",
    "                \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        D_loss_fake = loss_function(outputs, fake_labels)\n",
    "                \n",
    "        # Backprop and optimize\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_optimizer.zero_grad()       \n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # ========================================================= #\n",
    "        #                        Train the generator                #\n",
    "        # ========================================================= #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "    \n",
    "        G_loss = loss_function(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()      \n",
    "        \n",
    "    D_loss_epochs.append(D_loss.mean().item())\n",
    "    G_loss_epochs.append(G_loss.mean().item())       \n",
    "    \n",
    "    print('Epoch [%d/%d]: D_loss: %.4f, G_loss: %.4f' % (epoch, num_epochs, D_loss.mean().item(), G_loss.mean().item()))\n",
    "    \n",
    "#    print('Epoch [%d/%d]: D_loss: %.4f, G_loss: %.4f' % (epoch, num_epochs, torch.mean(torch.FloatTensor(D_loss)), \n",
    "#                                                   torch.mean(torch.FloatTensor(G_loss))))\n",
    "    # real image 저장\n",
    "    if (epoch+1) == num_epochs:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image((images+1)/2, os.path.join('./mnist_data/', 'real_images.png'))\n",
    "    \n",
    "    # 생성된 이미지 저장\n",
    "    if epoch==0:\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        save_image((fake_images+1)/2, os.path.join('./mnist_data/', 'fake_images-{}.png'.format(epoch+1)))\n",
    "        \n",
    "    if (epoch+1) % 10==0:\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        save_image((fake_images+1)/2, os.path.join('./mnist_data/', 'fake_images-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(D_loss_epochs, '-')\n",
    "plt.plot(G_loss_epochs, '-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('GAN Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100, 1])\n",
      "tensor(0.6527, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "torch.Size([100, 64])\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100, 1])\n",
      "tensor(0.7088, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "D_loss = tensor(1.3615, grad_fn=<AddBackward0>)\n",
      "torch.Size([100, 64])\n",
      "torch.Size([100, 784])\n",
      "D_output.shape = torch.Size([100, 1])\n",
      "G_loss = tensor(0.6789, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Epoch [0/1]: D_loss: 1.3615, G_loss: 0.6789\n",
      "100\n",
      "torch.Size([100, 784])\n",
      "torch.Size([100, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# prerequisites\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 64\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 1\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "\n",
    "# Data Loader \n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())\n",
    "\n",
    "D.to(device)\n",
    "G.to(device)\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0002 \n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n",
    "\n",
    "D_loss_epochs, G_loss_epochs = [], []\n",
    "for epoch in range(num_epochs):    \n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        print(images.shape) \n",
    "        images = images.reshape(BATCH_SIZE, -1).to(device)\n",
    "        print(images.shape) \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "        # ======================================================== #\n",
    "        #                      Train the discriminator             #\n",
    "        # ======================================================== #\n",
    "\n",
    "        outputs = D(images)\n",
    "        print(outputs.shape) \n",
    "        D_loss_real = loss_function(outputs, real_labels)\n",
    "        print(D_loss_real)         \n",
    "        #break\n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        print(z.shape)        \n",
    "        fake_images = G(z)\n",
    "        print(fake_images.shape)\n",
    "        outputs = D(fake_images)\n",
    "        print(outputs.shape)        \n",
    "        D_loss_fake = loss_function(outputs, fake_labels)\n",
    "        print(D_loss_fake)  \n",
    "        #break\n",
    "        # Backprop and optimize\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        print(\"D_loss =\",D_loss)  \n",
    "        D_optimizer.zero_grad()       \n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        #break\n",
    "        # ========================================================= #\n",
    "        #                        Train the generator                #\n",
    "        # ========================================================= #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        print(z.shape)\n",
    "        fake_images = G(z)\n",
    "        print(fake_images.shape)        \n",
    "        outputs = D(fake_images)\n",
    "        print(\"D_output.shape =\", outputs.shape)\n",
    "        G_loss = loss_function(outputs, real_labels)\n",
    "        print(\"G_loss =\", G_loss) \n",
    "        \n",
    "        # Backprop and optimize\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()      \n",
    "        break\n",
    "    D_loss_epochs.append(D_loss.mean().item())\n",
    "    G_loss_epochs.append(G_loss.mean().item())       \n",
    "    \n",
    "    print('Epoch [%d/%d]: D_loss: %.4f, G_loss: %.4f' % (epoch, num_epochs, D_loss.mean().item(), G_loss.mean().item()))\n",
    "    \n",
    "#    print('Epoch [%d/%d]: D_loss: %.4f, G_loss: %.4f' % (epoch, num_epochs, torch.mean(torch.FloatTensor(D_loss)), \n",
    "#                                                   torch.mean(torch.FloatTensor(G_loss))))\n",
    "    # real image 저장\n",
    "    if (epoch+1) == num_epochs:\n",
    "        print(images.size(0))\n",
    "        print(images.shape)\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        print(images.shape)\n",
    "        save_image((images+1)/2, os.path.join('./mnist_data/', 'real_images.png'))\n",
    "    \n",
    "    # 생성된 이미지 저장\n",
    "    if epoch==0:\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        save_image((fake_images+1)/2, os.path.join('./mnist_data/', 'fake_images-{}.png'.format(epoch+1)))\n",
    "        \n",
    "    if (epoch+1) % 10==0:\n",
    "        fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "        save_image((fake_images+1)/2, os.path.join('./mnist_data/', 'fake_images-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_scores_epochs, '-')\n",
    "plt.plot(fake_scores_epochs, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real Score', 'Fake score'])\n",
    "plt.title('Scores');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 100\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 1\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader \n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())\n",
    "\n",
    "D.to(device)\n",
    "G.to(device)\n",
    "\n",
    "# loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# optimizer\n",
    "learning_rate = 0.0002 \n",
    "D_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)\n",
    "G_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "D_loss_epochs, G_loss_epochs, real_scores_epochs, fake_scores_epochs = [], [], [], []\n",
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.reshape(BATCH_SIZE, -1).to(device)\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "        fake_labels = torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        outputs = D(images)\n",
    "        D_loss_real = loss_function(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        D_loss_fake = loss_function(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_optimizer.zero_grad()       \n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(BATCH_SIZE, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "    \n",
    "        G_loss = loss_function(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        G_optimizer.zero_grad()\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, D_loss.item(), G_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    D_loss_epochs.append(D_loss.mean().item())\n",
    "    G_loss_epochs.append(G_loss.mean().item())\n",
    "    real_scores_epochs.append(real_score.mean().item())            \n",
    "    fake_scores_epochs.append(fake_score.mean().item())  \n",
    "    \n",
    "    # real image 저장\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images), os.path.join('./mnist_data/', 'real_images.png'))\n",
    "    \n",
    "    # 생성된 이미지 저장\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), os.path.join('./mnist_data/', 'fake_images-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "latent_size = 100\n",
    "hidden_size = 256\n",
    "image_size = 784\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "sample_dir = 'samples'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory if not exists\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist = MNIST(root='data', \n",
    "              train=True, \n",
    "              download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(mnist[0])\n",
    "print(type(mnist[0][0]))\n",
    "image = np.array(mnist[0][0])\n",
    "print(type(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist = MNIST(root='data', \n",
    "              train=True, \n",
    "              download=True,\n",
    "              transform=ToTensor()   \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(mnist[0])\n",
    "print(type(mnist[0][0]))\n",
    "image = np.array(mnist[0][0])\n",
    "print(type(image))\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "mnist = MNIST(root='data', \n",
    "              train=True, \n",
    "              download=True,\n",
    "              transform=Compose([ToTensor(), Normalize(mean=(0.5,), std=(0.5,))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = mnist[0]\n",
    "print('Label: ', label)\n",
    "print(img[:,10:15,10:15])\n",
    "torch.min(img), torch.max(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "img_norm = denorm(img)\n",
    "plt.imshow(img_norm[0], cmap='gray')\n",
    "print('Label:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 100\n",
    "data_loader = DataLoader(mnist, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_batch, label_batch in data_loader:\n",
    "    print('first batch')\n",
    "    print(img_batch.shape)\n",
    "    plt.imshow(img_batch[0][0], cmap='gray')\n",
    "    print(label_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = G(torch.randn(2, latent_size))\n",
    "gen_imgs = denorm(y.reshape((-1, 28,28)).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gen_imgs[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(images):\n",
    "    # Create the labels which are later used as input for the BCE loss\n",
    "    real_labels = torch.ones(batch_size, 1).to(device)\n",
    "    fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "        \n",
    "    # Loss for real images\n",
    "    outputs = D(images)\n",
    "    d_loss_real = criterion(outputs, real_labels)\n",
    "    real_score = outputs\n",
    "\n",
    "    # Loss for fake images\n",
    "    z = torch.randn(batch_size, latent_size).to(device)\n",
    "    fake_images = G(z)\n",
    "    outputs = D(fake_images)\n",
    "    d_loss_fake = criterion(outputs, fake_labels)\n",
    "    fake_score = outputs\n",
    "\n",
    "    # Combine losses\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    # Reset gradients\n",
    "    reset_grad()\n",
    "    # Compute gradients\n",
    "    d_loss.backward()\n",
    "    # Adjust the parameters using backprop\n",
    "    d_optimizer.step()\n",
    "    \n",
    "    return d_loss, real_score, fake_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator():\n",
    "    # Generate fake images and calculate loss\n",
    "    z = torch.randn(batch_size, latent_size).to(device)\n",
    "    fake_images = G(z)\n",
    "    labels = torch.ones(batch_size, 1).to(device)\n",
    "    g_loss = criterion(D(fake_images), labels)\n",
    "\n",
    "    # Backprop and optimize\n",
    "    reset_grad()\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss, fake_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Save some real images\n",
    "for images, _ in data_loader:\n",
    "    images = images.reshape(images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'), nrow=10)\n",
    "    break\n",
    "   \n",
    "Image(os.path.join(sample_dir, 'real_images.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vectors = torch.randn(batch_size, latent_size).to(device)\n",
    "\n",
    "def save_fake_images(index):\n",
    "    fake_images = G(sample_vectors)\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    fake_fname = 'fake_images-{0:0=4d}.png'.format(index)\n",
    "    print('Saving', fake_fname)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, fake_fname), nrow=10)\n",
    "    \n",
    "# Before training\n",
    "save_fake_images(0)\n",
    "Image(os.path.join(sample_dir, 'fake_images-0000.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "num_epochs = 10\n",
    "total_step = len(data_loader)\n",
    "d_losses, g_losses, real_scores, fake_scores = [], [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        # Load a batch & transform to vectors\n",
    "        images = images.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        # Train the discriminator and generator\n",
    "        d_loss, real_score, fake_score = train_discriminator(images)\n",
    "        g_loss, fake_images = train_generator()\n",
    "        \n",
    "        # Inspect the losses\n",
    "        if (i+1) % 200 == 0:\n",
    "            d_losses.append(d_loss.item())\n",
    "            g_losses.append(g_loss.item())\n",
    "            real_scores.append(real_score.mean().item())\n",
    "            fake_scores.append(fake_score.mean().item())\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "        \n",
    "    # Sample and save images\n",
    "    save_fake_images(epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./samples/fake_images-0010.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_losses, '-')\n",
    "plt.plot(g_losses, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(real_scores, '-')\n",
    "plt.plot(fake_scores, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "plt.legend(['Real Score', 'Fake score'])\n",
    "plt.title('Scores');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://github.com/lyeoni/pytorch-mnist-GAN/blob/master/pytorch-mnist-GAN.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "# MNIST Dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super(Generator, self).__init__()       \n",
    "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        return torch.tanh(self.fc4(x))\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        return torch.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build network\n",
    "z_dim = 100\n",
    "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n",
    "\n",
    "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
    "D = Discriminator(mnist_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "# optimizer\n",
    "lr = 0.0002 \n",
    "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
    "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_train(x):\n",
    "    #=======================Train the discriminator=======================#\n",
    "    D.zero_grad()\n",
    "\n",
    "    # train discriminator on real\n",
    "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(BATCH_SIZE, 1)\n",
    "   \n",
    "    D_output = D(x_real)\n",
    "    D_real_loss = criterion(D_output, y_real)\n",
    "    D_real_score = D_output\n",
    "\n",
    "    # train discriminator on facke\n",
    "    z = Variable(torch.randn(BATCH_SIZE, z_dim).to(device))\n",
    "    x_fake, y_fake = G(z), torch.zeros(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "    D_output = D(x_fake)\n",
    "    D_fake_loss = criterion(D_output, y_fake)\n",
    "    D_fake_score = D_output\n",
    "\n",
    "    # gradient backprop & optimize ONLY D's parameters\n",
    "    D_loss = D_real_loss + D_fake_loss\n",
    "    D_loss.backward()\n",
    "    D_optimizer.step()\n",
    "        \n",
    "    return  D_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_train(x):\n",
    "    #=======================Train the generator=======================#\n",
    "    G.zero_grad()\n",
    "\n",
    "    z = torch.randn(BATCH_SIZE, z_dim).to(device)\n",
    "    y = torch.ones(BATCH_SIZE, 1).to(device)\n",
    "\n",
    "    G_output = G(z)\n",
    "    D_output = D(G_output)\n",
    "    G_loss = criterion(D_output, y)\n",
    "\n",
    "    # gradient backprop & optimize ONLY G's parameters\n",
    "    G_loss.backward()\n",
    "    G_optimizer.step()\n",
    "        \n",
    "    return G_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 10\n",
    "for epoch in range(1, n_epoch+1):           \n",
    "    D_losses, G_losses = [], []\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        D_losses.append(D_train(x))\n",
    "        G_losses.append(G_train(x))\n",
    "\n",
    "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
    "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    test_z = torch.randn(bs, z_dim).to(device)\n",
    "    generated = G(test_z)\n",
    "\n",
    "    save_image(generated.view(generated.size(0), 1, 28, 28), './samples/sample_' + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://ws-choi.github.io/blog-kor/seminar/tutorial/mnist/pytorch/gan/GAN-%ED%8A%9C%ED%86%A0%EB%A6%AC%EC%96%BC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "is_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if is_cuda else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardization code\n",
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.5),   \n",
    "                                         std=(0.5))])  \n",
    "\n",
    "# MNIST dataset\n",
    "train_data = dsets.MNIST(root='MNIST_data/', train=True, transform=transform, download=True)\n",
    "test_data  = dsets.MNIST(root='MNIST_data/', train=False, transform=transform, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(len(train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(\"---------------------------------\")\n",
    "print(test_data)\n",
    "print(len(test_data))\n",
    "print(test_data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 200\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_data_loader  = torch.utils.data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize    \n",
    "    np_img = img.numpy()    \n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)), cmap='gray' )\n",
    "    plt.show()\n",
    "\n",
    "print(train_data[0][0].shape)\n",
    "imshow(train_data[0][0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "def imshow_grid(img):\n",
    "    img = utils.make_grid(img)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "mini_batch_img, mini_batch_label  = next(iter(train_data_loader))\n",
    "print(mini_batch_img.shape)\n",
    "imshow_grid(mini_batch_img[0:16,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mini_batch_img, mini_batch_label  = next(iter(train_data_loader))\n",
    "\n",
    "mini_batch=16\n",
    "for i in range(mini_batch):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(mini_batch_img[i].squeeze(), cmap='gray')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_noise  = 100\n",
    "d_hidden = 256\n",
    "\n",
    "def sample_z(batch_size = 1, d_noise=100):\n",
    "    return torch.randn(batch_size, d_noise, device=device)\n",
    "\n",
    "G = nn.Sequential(\n",
    "    nn.Linear(d_noise, d_hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(d_hidden,d_hidden),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(d_hidden, 28*28),\n",
    "    nn.Tanh()\n",
    ").to(device)\n",
    "\n",
    "# 노이즈 생성하기\n",
    "z = sample_z()\n",
    "# 가짜 이미지 생성하기\n",
    "img_fake = G(z).view(-1,28,28)\n",
    "# 이미지 출력하기\n",
    "print(img_fake.shape)\n",
    "imshow(img_fake.detach())\n",
    "# Batch SIze만큼 노이즈 생성하여 그리드로 출력하기\n",
    "z = sample_z(BATCH_SIZE)\n",
    "img_fake = G(z)\n",
    "imshow_grid(img_fake.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = nn.Sequential(\n",
    "    nn.Linear(28*28, d_hidden),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(d_hidden, d_hidden),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(d_hidden, 1),\n",
    "    nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "print(G(z).shape)\n",
    "print(D(G(z)).shape)\n",
    "print(D(G(z)[0:5]).transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.BCELoss()\n",
    "\n",
    "def run_epoch(generator, discriminator, _optimizer_g, _optimizer_d):\n",
    "\n",
    "#    generator.train()\n",
    "#    discriminator.train()\n",
    "\n",
    "    for img_batch, label_batch in train_data_loader:\n",
    "\n",
    "        img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n",
    "\n",
    "        # ================================================  #\n",
    "        # maximize V(discriminator,generator) = optimize discriminator (setting k to be 1)  #\n",
    "        # ================================================  #\n",
    "\n",
    "        # init optimizer\n",
    "        _optimizer_d.zero_grad()\n",
    "\n",
    "        p_real = discriminator(img_batch.view(-1, 28*28))\n",
    "        p_fake = discriminator(generator(sample_z(BATCH_SIZE, d_noise)))\n",
    "\n",
    "        # ================================================  #\n",
    "        #    Loss computation (soley based on the paper)    #\n",
    "        # ================================================  #\n",
    "#        loss_real = -1 * torch.log(p_real)   # -1 for gradient ascending\n",
    "#        loss_fake = -1 * torch.log(1.-p_fake) # -1 for gradient ascending\n",
    "#        loss_d    = (loss_real + loss_fake).mean()\n",
    "        loss_d = loss_function(p_real, p_fake)\n",
    "        # ================================================  #\n",
    "        #     Loss computation (based on Cross Entropy)     #\n",
    "        # ================================================  #\n",
    "        # loss_d = criterion(p_real, torch.ones_like(p_real).to(device)) + \\    #\n",
    "        #          criterion(p_fake, torch.zeros_like(p_real).to(device))       #\n",
    "\n",
    "        # Update parameters\n",
    "        loss_d.backward()\n",
    "        _optimizer_d.step()\n",
    "\n",
    "        # ================================================  #\n",
    "        #        minimize V(discriminator,generator)        #\n",
    "        # ================================================  #\n",
    "\n",
    "        # init optimizer\n",
    "        _optimizer_g.zero_grad()\n",
    "\n",
    "        p_fake = discriminator(generator(sample_z(BATCH_SIZE, d_noise)))\n",
    "\n",
    "        # ================================================  #\n",
    "        #    Loss computation (soley based on the paper)    #\n",
    "        # ================================================  #\n",
    "\n",
    "        # instead of: torch.log(1.-p_fake).mean() <- explained in Section 3\n",
    "        loss_g = -1 * torch.log(p_fake).mean()\n",
    "\n",
    "        # ================================================  #\n",
    "        #     Loss computation (based on Cross Entropy)     #\n",
    "        # ================================================  #\n",
    "        # loss_g = criterion(p_fake, torch.ones_like(p_fake).to(device)) #\n",
    "\n",
    "        loss_g.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        _optimizer_g.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(generator, discriminator):\n",
    "\n",
    "    p_real, p_fake = 0.,0.\n",
    "\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "\n",
    "    for img_batch, label_batch in test_data_loader:\n",
    "\n",
    "        img_batch, label_batch = img_batch.to(device), label_batch.to(device)\n",
    "\n",
    "        with torch.autograd.no_grad():\n",
    "            p_real += (torch.sum(discriminator(img_batch.view(-1, 28*28))).item())/10000.\n",
    "            p_fake += (torch.sum(discriminator(generator(sample_z(BATCH_SIZE, d_noise)))).item())/10000.\n",
    "\n",
    "\n",
    "    return p_real, p_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = optim.Adam(G.parameters(), lr = 0.0002)\n",
    "optimizer_d = optim.Adam(D.parameters(), lr = 0.0002)\n",
    "\n",
    "p_real_trace = []\n",
    "p_fake_trace = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    run_epoch(G, D, optimizer_g, optimizer_d)\n",
    "    p_real, p_fake = evaluate_model(G,D)\n",
    "\n",
    "    p_real_trace.append(p_real)\n",
    "    p_fake_trace.append(p_fake)\n",
    "\n",
    "    if((epoch+1)% 5 == 0):\n",
    "        print('(epoch %i/200) p_real: %f, p_g: %f' % (epoch+1, p_real, p_fake))\n",
    "        imshow_grid(G(sample_z(16)).view(-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(model):\n",
    "    for p in model.parameters():\n",
    "        if(p.dim() > 1):\n",
    "            nn.init.xavier_normal_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p, 0.1, 0.2)\n",
    "\n",
    "init_params(G)\n",
    "init_params(D)\n",
    "\n",
    "optimizer_g = optim.Adam(G.parameters(), lr = 0.0002)\n",
    "optimizer_d = optim.Adam(D.parameters(), lr = 0.0002)\n",
    "\n",
    "p_real_trace = []\n",
    "p_fake_trace = []\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    run_epoch(G, D, optimizer_g, optimizer_d)\n",
    "    p_real, p_fake = evaluate_model(G,D)\n",
    "\n",
    "    p_real_trace.append(p_real)\n",
    "    p_fake_trace.append(p_fake)\n",
    "\n",
    "    if((epoch+1)% 5 == 0):\n",
    "        print('(epoch %i/200) p_real: %f, p_g: %f' % (epoch+1, p_real, p_fake))\n",
    "        imshow_grid(G(sample_z(16)).view(-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(p_fake_trace, label='D(x_generated)')\n",
    "plt.plot(p_real_trace, label='D(x_real)')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://kingnamji.tistory.com/11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as utils\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device = {device}\")\n",
    "sample_dir = 'samples'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "latent_size = 100\n",
    "hidden_size = 256\n",
    "image_size = 784 # 28 * 28\n",
    "batch_size = 100\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Processing\n",
    "transform = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5], # 1 for gray scale 만약, RGB channels라면 mean=(0.5, 0.5, 0.5)\n",
    "                                         std=[0.5])])  # 1 for gray scale 만약, RGB channels라면 std=(0.5, 0.5, 0.5)\n",
    "\n",
    "# MNIST 데이터셋\n",
    "mnist_train = dsets.MNIST(root='data/',\n",
    "                         train=True, # 트레인 셋\n",
    "                         transform=transform,\n",
    "                         download=True)\n",
    "mnist_test  = dsets.MNIST(root='data/', \n",
    "                          train=False,\n",
    "                          transform=transform,\n",
    "                          download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize      \n",
    "    np_img = img.numpy()    \n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)), cmap='gray' )\n",
    "    plt.show()\n",
    "\n",
    "print(mnist_train[0][0].shape)\n",
    "imshow(mnist_train[0][0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, # 훈련용 데이터 로딩\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True) # 에폭마다 데이터 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "def imshow_grid(img):\n",
    "    img = utils.make_grid(img)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "mini_batch_img, mini_batch_label  = next(iter(data_loader))\n",
    "print(mini_batch_img.shape)\n",
    "imshow_grid(mini_batch_img[0:16,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid()) # Binary Cross Entropy loss 를 사용할 것이기에 sigmoid 사용!\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device setting\n",
    "D = D.to(device)\n",
    "G = G.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자 이용해 noise 만들기\n",
    "noise = torch.randn(1, 100, device=device)\n",
    "fake_image = G(noise).view(-1,28,28)\n",
    "print(fake_image.shape)\n",
    "imshow(fake_image.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch SIze만큼 노이즈 생성하여 그리드로 출력\n",
    "noise = torch.randn(batch_size, 100, device=device)\n",
    "img_fake = G(noise)\n",
    "imshow_grid(img_fake.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(G(noise).shape)\n",
    "print(D(G(noise)).shape)\n",
    "print(D(G(noise)[0:5]).transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "loss_function = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)\n",
    "\n",
    "\n",
    "def reset_grad(): # 가중치를 0으로 초기화\n",
    "    d_optimizer.zero_grad()\n",
    "    g_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_loss_epochs, g_loss_epochs, real_scores_epochs, fake_scores_epochs = [], [], [], []\n",
    "total_step = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, _) in enumerate(data_loader):\n",
    "        images = images.reshape(batch_size, -1).to(device)\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                      Train the discriminator                       #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = D(images)\n",
    "        d_loss_real = loss_function(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "        d_loss_fake = loss_function(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()       \n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        # ================================================================== #\n",
    "        #                        Train the generator                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # Compute loss with fake images\n",
    "        z = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images)\n",
    "    \n",
    "        g_loss = loss_function(outputs, real_labels)\n",
    "        \n",
    "        # Backprop and optimize\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}' \n",
    "                  .format(epoch, num_epochs, i+1, total_step, d_loss.item(), g_loss.item(), \n",
    "                          real_score.mean().item(), fake_score.mean().item()))\n",
    "    \n",
    "    d_loss_epochs.append(d_loss.mean().item())\n",
    "    g_loss_epochs.append(g_loss.mean().item())\n",
    "    real_scores_epochs.append(real_score.mean().item())            \n",
    "    fake_scores_epochs.append(fake_score.mean().item())\n",
    "    # real image 저장\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.reshape(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images), os.path.join(sample_dir, 'real_images.png'))\n",
    "    \n",
    "    # 생성된 이미지 저장\n",
    "    fake_images = fake_images.reshape(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images), os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch+1)))\n",
    "\n",
    "# 생성자, 판별자 각각 모델 저장\n",
    "torch.save(G.state_dict(), 'G.ckpt')\n",
    "torch.save(D.state_dict(), 'D.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(d_loss_epochs, '-')\n",
    "plt.plot(g_loss_epochs, '-')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['Discriminator', 'Generator'])\n",
    "plt.title('Losses');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot    \n",
    "plt.figure(figsize = (12, 8))\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('score')\n",
    "x = np.arange(num_epochs)\n",
    "plt.plot(x, real_scores_epochs, 'g', label='D(x)')\n",
    "plt.plot(x, fake_scores_epochs, 'b', label='D(G(z))')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST(\"./data\", train=True, download=True, \n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = trainset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._layers = nn.Sequential(\n",
    "            nn.Linear(z_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),    \n",
    "            nn.Linear(512, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self._layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._layers = nn.Sequential(\n",
    "            nn.Linear(784, 512),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.25),    \n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.25),\n",
    "            nn.Linear(128,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self._layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "real_labels = torch.ones(BATCH_SIZE, 1)\n",
    "fake_labels = torch.zeros(BATCH_SIZE, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_size = 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = DNet()\n",
    "G = GNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4,3)\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        \n",
    "        images = images.reshape(BATCH_SIZE, -1)\n",
    "        outputs = D(images)\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        z = torch.randn(BATCH_SIZE, z_size)\n",
    "        fake_images = G(z)\n",
    "        outputs = D(fake_images.detach())\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        outputs = D(fake_images)\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "    print('Epoch[{:3d}/{:3d}] d_loss: {:.4f}, g_loss: {:.4f}, D(x): {:.2f}, D(G(z)): {:.2f}'.format(\n",
    "        epoch, EPOCHS, d_loss.item(), g_loss.item(), real_score.mean().item(), fake_score.mean().item()))\n",
    "    z = torch.randn(BATCH_SIZE, z_size)\n",
    "    fake_images = G(z)\n",
    "    for row in range(4):\n",
    "        for col in range(3):\n",
    "            fake_images_img = np.reshape(fake_images.data.cpu().numpy()[row*4+col],(28,28))\n",
    "            axis = axes[row][col]\n",
    "            axis.get_xaxis().set_ticks([])\n",
    "            axis.get_yaxis().set_ticks([])\n",
    "            axis.imshow(fake_images_img, cmap='gray')\n",
    "    \n",
    "    plt.savefig('./data/DNN_GAN_{:03d}.jpg'.format(epoch)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 손실 함수와 최적화 기법 지정하기\n",
    "# Binary Cross Entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 생성자의 매개 변수를 최적화하는 Adam optimizer\n",
    "G_optimizer = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "# 구분자의 매개 변수를 최적화하는 Adam optimizer\n",
    "D_optimizer = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 랜덤으로 9개만 시각화\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(mnist_train), size=(1,)).item()\n",
    "    img, label = mnist_train[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis(\"off\") # x축, y축 안보이게 설정\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, # 훈련용 데이터 로딩\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True) # 에폭마다 데이터 섞기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "D = nn.Sequential(\n",
    "    nn.Linear(image_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(hidden_size, 1),\n",
    "    nn.Sigmoid()) # Binary Cross Entropy loss 를 사용할 것이기에 sigmoid 사용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(latent_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, hidden_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_size, image_size),\n",
    "    nn.Tanh())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = (img+1) / 2\n",
    "    img = img.squeeze() # 차원 중 사이즈 1 을 제거\n",
    "    np_img = img.numpy() # 이미지 픽셀을 넘파이 배열로 변환\n",
    "    plt.imshow(np_img,cmap='gray')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_grid(img): \n",
    "    img = utils.make_grid(img.cpu().detach()) # 이미지 그리드 생성, 이미지 출력만을 위해 cpu에 담고 추적 방이\n",
    "    img = (img+1)/2\n",
    "    npimg = img.numpy() # 이미지 픽셀을 넘파이 배열로 변환\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성자 이용해 데이터 만들기\n",
    "rand = torch.randn(1, 100)\n",
    "img_1 = G(rand).view(-1,28,28)\n",
    "\n",
    "imshow(img_1.squeeze().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
