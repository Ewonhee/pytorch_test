{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3472.784088656839\n",
      "199 2378.646500085366\n",
      "299 1631.9705040417275\n",
      "399 1121.8293258364363\n",
      "499 772.891757591188\n",
      "599 533.9432505377972\n",
      "699 370.12620380130966\n",
      "799 257.6890498290987\n",
      "899 180.429306309487\n",
      "999 127.2815487878463\n",
      "1099 90.68000153979266\n",
      "1199 65.44575661980836\n",
      "1299 48.029686584806434\n",
      "1399 35.99677519154814\n",
      "1499 27.674484173970338\n",
      "1599 21.9127014549904\n",
      "1699 17.919675209089945\n",
      "1799 15.149752266761617\n",
      "1899 13.226475159035143\n",
      "1999 11.889840618714494\n",
      "Result: y = 0.051795427535165514 + 0.8314254489701306 x + -0.008935573129450475 x^2 + -0.08972949467774548 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.9431]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.6558], requires_grad=True)]\n",
      "Epoch    0/3000 Cost: 48.572453\n",
      "Epoch  100/3000 Cost: 0.025402\n",
      "Epoch  200/3000 Cost: 0.015697\n",
      "Epoch  300/3000 Cost: 0.009700\n",
      "Epoch  400/3000 Cost: 0.005994\n",
      "Epoch  500/3000 Cost: 0.003704\n",
      "Epoch  600/3000 Cost: 0.002289\n",
      "Epoch  700/3000 Cost: 0.001414\n",
      "Epoch  800/3000 Cost: 0.000874\n",
      "Epoch  900/3000 Cost: 0.000540\n",
      "Epoch 1000/3000 Cost: 0.000334\n",
      "Epoch 1100/3000 Cost: 0.000206\n",
      "Epoch 1200/3000 Cost: 0.000127\n",
      "Epoch 1300/3000 Cost: 0.000079\n",
      "Epoch 1400/3000 Cost: 0.000049\n",
      "Epoch 1500/3000 Cost: 0.000030\n",
      "Epoch 1600/3000 Cost: 0.000019\n",
      "Epoch 1700/3000 Cost: 0.000011\n",
      "Epoch 1800/3000 Cost: 0.000007\n",
      "Epoch 1900/3000 Cost: 0.000004\n",
      "Epoch 2000/3000 Cost: 0.000003\n",
      "Epoch 2100/3000 Cost: 0.000002\n",
      "Epoch 2200/3000 Cost: 0.000001\n",
      "Epoch 2300/3000 Cost: 0.000001\n",
      "Epoch 2400/3000 Cost: 0.000000\n",
      "Epoch 2500/3000 Cost: 0.000000\n",
      "Epoch 2600/3000 Cost: 0.000000\n",
      "Epoch 2700/3000 Cost: 0.000000\n",
      "Epoch 2800/3000 Cost: 0.000000\n",
      "Epoch 2900/3000 Cost: 0.000000\n",
      "Epoch 3000/3000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    " \n",
    "# 데이터 생성 \n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    " \n",
    "# 모델을 선언 및 초기화 \n",
    "model = nn.Linear(1,1)\n",
    " \n",
    "#모델 파라미터 출력\n",
    "print(list(model.parameters()))\n",
    " \n",
    "#옵티마이저 설정 \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    " \n",
    "#학습 : 경사하강법 \n",
    "epochs = 3000\n",
    "for epoch in range(epochs+1):\n",
    "    prediction = model(x_train)  #H(x)계산, 클래스에 그냥 입력하면 됨 \n",
    "    \n",
    "    cost = F.mse_loss(prediction, y_train) #cost 계산 \n",
    "    \n",
    "    #cost로 H(x) 개선 \n",
    "    optimizer.zero_grad()  #gradient=0 초기\n",
    "    cost.backward()  # 비용함수 미분해서 gradient 계산 \n",
    "    optimizer.step()  # 파라미터 업데이트 \n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 357.70330810546875\n",
      "199 248.1020965576172\n",
      "299 173.11932373046875\n",
      "399 121.76091003417969\n",
      "499 86.54341125488281\n",
      "599 62.366546630859375\n",
      "699 45.75006866455078\n",
      "799 34.31718826293945\n",
      "899 26.44194793701172\n",
      "999 21.011404037475586\n",
      "1099 17.262649536132812\n",
      "1199 14.672078132629395\n",
      "1299 12.880046844482422\n",
      "1399 11.63912582397461\n",
      "1499 10.778993606567383\n",
      "1599 10.182223320007324\n",
      "1699 9.767788887023926\n",
      "1799 9.479717254638672\n",
      "1899 9.279304504394531\n",
      "1999 9.139759063720703\n",
      "2099 9.042513847351074\n",
      "2199 8.974692344665527\n",
      "2299 8.927353858947754\n",
      "2399 8.894292831420898\n",
      "2499 8.871182441711426\n",
      "2599 8.855020523071289\n",
      "2699 8.843708992004395\n",
      "2799 8.835786819458008\n",
      "2899 8.830236434936523\n",
      "2999 8.82634449005127\n",
      "3099 8.823615074157715\n",
      "3199 8.821700096130371\n",
      "3299 8.820353507995605\n",
      "3399 8.819408416748047\n",
      "3499 8.818743705749512\n",
      "3599 8.818276405334473\n",
      "3699 8.817949295043945\n",
      "3799 8.817717552185059\n",
      "3899 8.817554473876953\n",
      "3999 8.817440032958984\n",
      "4099 8.817359924316406\n",
      "4199 8.817303657531738\n",
      "4299 8.81726360321045\n",
      "4399 8.817234992980957\n",
      "4499 8.817214965820312\n",
      "4599 8.817200660705566\n",
      "4699 8.817191123962402\n",
      "4799 8.817182540893555\n",
      "4899 8.817179679870605\n",
      "4999 8.817174911499023\n",
      "Result: y = 9.525469067739323e-05 + 0.8567265868186951 x + -1.6432457414339297e-05 x^2 + -0.09332836419343948 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성합니다\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치(기울기)를 초기화합니다\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(5000):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "   \n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 343.1071472167969\n",
      "199 230.51002502441406\n",
      "299 155.86732482910156\n",
      "399 106.37649536132812\n",
      "499 73.55638885498047\n",
      "599 51.787410736083984\n",
      "699 37.345603942871094\n",
      "799 27.762496948242188\n",
      "899 21.402023315429688\n",
      "999 17.179405212402344\n",
      "1099 14.375329971313477\n",
      "1199 12.51278018951416\n",
      "1299 11.275218963623047\n",
      "1399 10.452695846557617\n",
      "1499 9.905832290649414\n",
      "1599 9.5421142578125\n",
      "1699 9.30013370513916\n",
      "1799 9.139077186584473\n",
      "1899 9.031829833984375\n",
      "1999 8.96038818359375\n",
      "Result: y = -0.004667290486395359 + 0.8459258675575256 x + 0.0008051855838857591 x^2 + -0.0917920470237732 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성합니다.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주할 수 있습니다.\n",
    "# (x, x^2, x^3)를 위한 텐서를 준비합니다.\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,\n",
    "# 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻습니다.\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의합니다.\n",
    "# nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 \n",
    "# 출력을 생성합니다. 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터\n",
    "# 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장합니다.\n",
    "# Flatten 계층은 선형 계층의 출력을 `y` 의 shape과 맞도록(match) 1D 텐서로 폅니다(flatten).\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 주로 사용되는 손실 함수(loss function)들에 대한 정의도 포함되어 있습니다;\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용하겠습니다.\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # 순전파 단계: x를 모델에 전달하여 예측값 y를 계산합니다. Module 객체는 __call__ 연산자를 \n",
    "    # 덮어써서(override) 함수처럼 호출할 수 있도록 합니다. 이렇게 함으로써 입력 데이터의 텐서를 Module에 전달하여\n",
    "    # 출력 데이터의 텐서를 생성합니다.\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다. 예측한 y와 정답인 y를 갖는 텐서들을 전달하고,\n",
    "    # 손실 함수는 손실(loss)을 갖는 텐서를 반환합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 만듭니다.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산합니다.\n",
    "    # 내부적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,\n",
    "    # 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법을 사용하여 가중치를 갱신합니다.\n",
    "    # 각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있습니다.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# list의 첫번째 항목에 접근하는 것처럼 `model` 의 첫번째 계층(layer)에 접근할 수 있습니다.\n",
    "linear_layer = model[0]\n",
    "\n",
    "# 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됩니다.\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e7e9642bb2133bb3e9049961efa42b00dd232b34cd2782d376eb5751d417706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
